# -*- coding: utf-8 -*-
"""sent_bert_mnr_cli5_v2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e4SQJ3g1ds_KT7zVR95TDRz_uGeXpiye
"""

# !pip install --quiet datasets
!pip install --quiet scann
!pip install --quiet datasets
!pip install --quiet pipreqsnb
import pandas as pd 
import numpy as np 
# from google.colab import drive
# drive.mount('/content/drive')
import json
import os 
import gzip
# import datasets 
from sklearn.model_selection import train_test_split
import tensorflow as tf 
from tensorflow import keras 
from tensorflow.keras import layers, Input, Model
from tensorflow.keras.layers import *
import tensorflow_hub as hub 

from sklearn.metrics import confusion_matrix

import scann
from data_loader import data_loader
from negative_maker import negative_maker
from model import model

train_df = data_loader('train_eli5').frame_maker()
test_df = data_loader('test_eli5').frame_maker()

# eli5 = datasets.load_dataset('eli5', split = 'train_eli5')

use_hub = hub.load("https://tfhub.dev/google/universal-sentence-encoder/4") ## universal sentence encoder model

train_df

train_df = negative_maker(train_df).neg_maker()

# train_df_l = pd.concat([train_neg_1,train_neg_2,train_neg_1])

print(len(train_df))

def distance_calc(y_true, y_pred):
    anchor, positive, negative = tf.split(y_pred, 3, axis=1)
    ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)
    an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)
    loss = ap_distance - an_distance
    margin = 0
    loss = tf.maximum(loss + margin, 0.0)
    return loss
    # return (ap_distance, an_distance)

print(len(train_df))
print(len(test_df))

triplet_model = model()

triplet_model.compile(
    optimizer = 'Adam',
    loss = distance_calc
)
y_dummy = np.ones(len(train_df)).reshape(-1,1)
triplet_model.fit([np.array(train_df['title']),
                   np.array(train_df['first_answer']),
                   np.array(train_df['neg_answer'])
                   ],
                   y_dummy,
                   epochs = 4,
                  batch_size = 64*64
                  )

use_emb =  triplet_model.get_layer('sentence_encoder')

q_0 = []
use_emb_test = []
for i in range(len(test_df)):
    # test_quest = test_df['title']
    test_answer = test_df['first_answer'][i]
    y = np.array(use_emb(([test_answer]))).reshape(1,512)

    use_emb_test.append(y)
    # q_0.append(tf.keras.layers.Dot(axes = 1, normalize = True)([test_quest_emb,y]))

use_emb_test = np.squeeze(np.array(use_emb_test), axis  =1)

searcher = scann.scann_ops_pybind.builder(use_emb_test, 40, "dot_product").tree(
    num_leaves=2000, num_leaves_to_search=100, training_sample_size=250000).score_ah(
    2, anisotropic_quantization_threshold=0.2).reorder(100).build()

test_quest = test_df['title'][0]
test_quest_emb = np.array(use_emb(([test_quest]))).reshape(1,512)

index, distance = searcher.search(test_quest_emb.ravel())
index

close_index = []
for i in range(len(test_df)):
    var1 = test_df['title'][i]
    var1 = np.array(use_emb(([var1]))).reshape(1,512)
    index, distance = searcher.search(var1.ravel())
    close_index.append(index)

# !pipreqsnb .

def _compute_precision_recall(targets, predictions, k):

    pred = predictions[:k]
    num_hit = len(set(pred).intersection(set(targets)))
    precision = float(num_hit) / len(pred)
    recall = float(num_hit) / len(targets)
    return precision, recall

N = [1,3,5,10,20]
for t in N:
    precisions = []
    recalls = []
    for i, _k in enumerate(close_index):
        precision, recall = _compute_precision_recall([i], _k,t)
    # print(precision)
        precisions.append(precision)
        recalls.append(recall)

    print('precision @',t, np.mean(precisions))
    print('recalls @',t, np.mean(recalls))
    print(' ')

