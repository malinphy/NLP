# -*- coding: utf-8 -*-
"""sent_bert_mnr_cli5_v2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e4SQJ3g1ds_KT7zVR95TDRz_uGeXpiye
"""

!pip install --quiet datasets
!pip install --quiet scann
import pandas as pd 
import numpy as np 
# from google.colab import drive
# drive.mount('/content/drive')
import json
import os 
import gzip
import datasets 
from sklearn.model_selection import train_test_split
import tensorflow as tf 
from tensorflow import keras 
from tensorflow.keras import layers, Input, Model
from tensorflow.keras.layers import *
import tensorflow_hub as hub 

from sklearn.metrics import confusion_matrix

import scann

eli5 = datasets.load_dataset('eli5', split = 'train_eli5')

use_hub = hub.load("https://tfhub.dev/google/universal-sentence-encoder/4") ## universal sentence encoder model

print('title length :', len(eli5['title']))
print('selftext length :', len(eli5['selftext']))
print('answers length :', len(eli5['answers']))

df = pd.DataFrame({'title':eli5['title'], 'selftext':eli5['selftext'], 'answer':eli5['answers']})
### as you can see from the data, dataset is corrupted, to eliminate
df.head(4)

answer_len = []
first_answer = []
for i in range(len(df)):
    answer_len.append(len(df['answer'][i]['text']))
    first_answer.append(df['answer'][i]['text'][0])

df['first_answer'] = first_answer


unique_answer = df['first_answer'].unique()
num_unique_answer = len(unique_answer)

unique_questions = df['title'].unique()
num_unique_questions =  len(unique_questions)

print('number of unique answers',num_unique_answer)
print('number of unique questions',num_unique_questions)

neg_pos = []
neg_title = []
neg_answer = []
for i in range(len(df)):
    x = np.random.randint(0, len(df))
    neg_pos.append(x)
    neg_title.append(df['title'][x])
    neg_answer.append(df['first_answer'][x])

df['neg_title'] = neg_title
df['neg_answer'] = neg_answer

def distance_calc(y_true, y_pred):
    anchor, positive, negative = tf.split(y_pred, 3, axis=1)
    ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)
    an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)
    loss = ap_distance - an_distance
    margin = 0
    loss = tf.maximum(loss + margin, 0.0)
    return loss
    # return (ap_distance, an_distance)

df,df_long = train_test_split(df,train_size= 0.25, test_size= 0.75, random_state = 42)
del df_long
train_df, test_df = train_test_split(df, train_size = 0.9, test_size = 0.1, random_state = 42)

train_df = train_df.reset_index(drop = True)
test_df = test_df.reset_index(drop = True)

print(len(train_df))
print(len(test_df))

anc_inp = Input(shape =(), dtype = tf.string, name = 'anchor_input')
pos_inp = Input(shape =(), dtype = tf.string, name = 'positive_input')
neg_inp = Input(shape =(), dtype = tf.string, name = 'negative_input')

use_emb = hub.KerasLayer(use_hub, trainable =True)

anc_emb = use_emb(anc_inp)
pos_emb = use_emb(pos_inp)
neg_emb = use_emb(neg_inp)

# d1_anc = Dense(256, activation = 'relu')(anc_emb)
# d1_pos = Dense(256, activation = 'relu')(pos_emb)
# d1_neg = Dense(256, activation = 'relu')(neg_emb)

final = tf.keras.layers.Concatenate(axis=-1)([anc_emb, pos_emb, neg_emb])
final = Dropout(0.2)(final)

triplet_model = Model(inputs = [anc_inp, pos_inp, neg_inp], outputs = final)

triplet_model.compile(
    optimizer = 'Adam',
    loss = distance_calc
)
y_dummy = np.ones(len(df)).reshape(-1,1)
triplet_model.fit([np.array(df['title']),
                   np.array(df['first_answer']),
                   np.array(df['neg_answer'])
                   ],
                   y_dummy,
                   epochs = 2,
                  batch_size = 32*8
                  )

test_df['title'][0]

test_quest = test_df['title'][8]
test_quest_emb = np.array(use_emb(([test_quest]))).reshape(1,512)
# use_emb(([test_quest]))

q_0 = []
use_emb_test = []
for i in range(len(test_df)):
    # test_quest = test_df['title']
    test_answer = test_df['first_answer'][i]
    y = np.array(use_emb(([test_answer]))).reshape(1,512)

    use_emb_test.append(y)
    q_0.append(tf.keras.layers.Dot(axes = 1, normalize = True)([test_quest_emb,y]))

tf.math.top_k(tf.squeeze(tf.squeeze(q_0, axis = -1), axis=-1), k =20)[1]

use_emb_test = np.squeeze(np.array(use_emb_test), axis = 1)

use_emb_test.shape

searcher = scann.scann_ops_pybind.builder(use_emb_test, 20, "dot_product").tree(
    num_leaves=2000, num_leaves_to_search=100, training_sample_size=250000).score_ah(
    2, anisotropic_quantization_threshold=0.2).reorder(100).build()

index, distance = searcher.search(test_quest_emb.ravel())
index

test_quest_emb.ravel().shape

