# -*- coding: utf-8 -*-
"""empty_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10sySnP4T8Ir7dA-x9d8zUiwjH-pKQee5
"""

!pip install --quiet tensorflow-text
!pip install --quiet tokenizers

import os 
import json
# from google.colab import files 
from google.colab import drive
drive.mount('/content/drive')
import pandas as pd
import numpy as np
import re

import numpy as np 
import pandas as pd
import os 
import sys

import matplotlib.pyplot as plt 
import tensorflow as tf 
from tensorflow import keras
from tensorflow.keras import layers,Input,Model
from tensorflow.keras.layers import * 
from tensorflow.keras.preprocessing.sequence import pad_sequences

import tensorflow_hub as hub 
from tokenizers import BertWordPieceTokenizer
# import tensorflow.compat.v1 as tf
import tensorflow_text as text 

bert_layer = hub.KerasLayer("https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2", trainable=True)
vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy().decode("utf-8")
tokenizer = BertWordPieceTokenizer(vocab=vocab_file, lowercase=True)

from data_loader import df_maker,answer_extractor
input_len = 384

train_path = 'drive/MyDrive/Colab Notebooks/datasets/squad/train-v1.1.json'
test_path = 'drive/MyDrive/Colab Notebooks/datasets/squad/dev-v1.1.json'

train_data = df_maker(train_path).squad_json_to_dataframe()
test_data = df_maker(test_path).squad_json_to_dataframe()


train_answer_text,train_answer_pos = answer_extractor(train_data['answers']).answer_extractor_2()
test_answer_text,test_answer_pos = answer_extractor(test_data['answers']).answer_extractor_2()

train_data = train_data.drop(columns = ['id', 'c_id'])
train_data['answers'] = train_answer_text
train_data['starting_idx'] = train_answer_pos

test_data = test_data.drop(columns = ['id', 'c_id'])
test_data['answers'] = test_answer_text
test_data['starting_idx'] = test_answer_pos
train_data = train_data.reset_index(drop=True)
test_data = test_data.reset_index(drop=True)

class str_maker :

  def __init__(self, data):
    self.data = data 

  def splitter(self):
    bos = []
    for i in self.data:
      bos.append(' '.join(i.split()))

    return bos

train_answer_str = str_maker(train_data['answers']).splitter()
train_question_str = str_maker(train_data['question']).splitter()
train_context_str = str_maker(train_data['context']).splitter()

test_answer_str = str_maker(test_data['answers']).splitter()
test_question_str = str_maker(test_data['question']).splitter()
test_context_str = str_maker(test_data['context']).splitter()


def token_positioner(x):
    first_pos = []
    last_pos = []
    for i in range(len(x)):
        context_ids = tokenizer.encode(x['context'][i]).ids
        answer_ids = tokenizer.encode(x['answers'][i]).ids[1:-1]
        pos = (np.where(np.in1d(context_ids,answer_ids)== True)[0])
        # print(i)
        if len(pos) != 0 :
            first_pos.append(pos[0])
        # last_pos_train.append(pos[-1])
            last_pos.append(pos[0]+len(answer_ids))

        else :
            first_pos.append(-1)
            last_pos.append(-1)
        # pos.append

    return first_pos, last_pos

first_pos_train,last_pos_train = token_positioner(train_data)
first_pos_test,last_pos_test = token_positioner(test_data)

dropper = np.where(np.array(first_pos_train) == -1)[0]
dropper2 = np.where(np.array(last_pos_train) > input_len-1)[0]
train_data  = train_data.drop(dropper)
train_data  = train_data.drop(dropper2).reset_index(drop = True)

train_answer_str = str_maker(train_data['answers']).splitter()
train_question_str = str_maker(train_data['question']).splitter()
train_context_str = str_maker(train_data['context']).splitter()

test_answer_str = str_maker(test_data['answers']).splitter()
test_question_str = str_maker(test_data['question']).splitter()
test_context_str = str_maker(test_data['context']).splitter()

first_pos_train,last_pos_train = token_positioner(train_data)
first_pos_test,last_pos_test = token_positioner(test_data)

train_data['first_pos'] = first_pos_train
train_data['last_pos'] = last_pos_train

test_data['first_pos'] = first_pos_test
test_data['last_pos'] = last_pos_test

def encoder(x,y):
    tokenizer = BertWordPieceTokenizer(vocab=vocab_file, lowercase=True)
    ids = []
    type_ids = []
    # tokens = []
    # offsets = []
    attention_mask = []
    # special_tokens_mask =[]
    ids_len = []
    for i in range(len(x)):
        var1 = tokenizer.encode(x[i],y[i])
        ids.append(var1.ids)
        type_ids.append(var1.type_ids)
        # tokens.append(var1.tokens)
        # offsets.append(var1.offsets)
        attention_mask.append(var1.attention_mask)
        # special_tokens_mask.append(var1.special_tokens_mask)
        ids_len.append(len(var1.ids))
    return ids, type_ids, attention_mask,ids_len

train_ids, train_type_ids, train_attention_mask, train_ids_len = encoder(train_context_str,train_answer_str)
test_ids, test_type_ids , test_attention_mask,  test_ids_len = encoder(test_context_str,test_answer_str)

def padder(x,pad_len):
    padded_var = pad_sequences(
    x,
    maxlen=pad_len,
    dtype='int32',
    padding='post',
    truncating='post',
    value=0.0
    )
    return padded_var

pad_ids_train = padder(train_ids,input_len)
pad_type_ids_train = padder(train_type_ids,input_len)
pad_attention_mask_train = padder(train_attention_mask,input_len)
# pad_special_tokens_mask_train = padder(train_special_tokens_mask, input_len)

pad_ids_test = padder(test_ids,input_len)
pad_type_ids_test = padder(test_type_ids,input_len)
pad_attention_mask_test = padder(test_attention_mask,input_len)
# pad_special_tokens_mask_test = padder(test_special_tokens_mask, input_len)


def squad_model():
    input_word_ids = Input(shape=(input_len,), dtype=tf.int32, name='input_word_ids')
    input_mask = Input(shape=(input_len,), dtype=tf.int32, name='input_mask')
    input_type_ids = Input(shape=(input_len,), dtype=tf.int32, name='input_type_ids')

    bert_layer = hub.KerasLayer("https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2", trainable=True)
    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids])

    start_flatten = Flatten()(sequence_output)
    start_final = Dense(input_len, activation = 'softmax')(start_flatten)

    end_flatten = Flatten()(sequence_output)
    end_final = Dense(input_len, activation = 'softmax')(start_flatten)


    return  Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=[start_final, end_final])

model = squad_model()

loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)
optimizer = keras.optimizers.Adam(learning_rate=1e-5, beta_1=0.9, beta_2=0.98, epsilon=1e-9)
model.compile(optimizer=optimizer, loss=[loss, loss],
              metrics = ['accuracy'])

history = model.fit([
            (pad_ids_train[0:300]), 
            (pad_attention_mask_train[0:300]), 
            (pad_type_ids_train[0:300])
            ],
            ([

            np.array(first_pos_train[0:300]),
            np.array(last_pos_train[0:300])
            ]),
            epochs = 1,
            batch_size = 4,
            )

preds_start, preds_end = model.predict(
            [
            (pad_ids_test[0:20]), 
            (pad_attention_mask_test[0:20]), 
            (pad_type_ids_test[0:20])
            ])

num = 19

tokenizer.encode(test_data['context'][num]).tokens[np.argmax(preds_start[num]):np.argmax(preds_end[num])]





