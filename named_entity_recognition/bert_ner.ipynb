{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dutDYGmgNLX",
        "outputId": "bb87d318-c127-47a1-c3cb-dde3c49a361e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet tensorflow-text\n",
        "!pip install --quiet tokenizers\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os \n",
        "import re\n",
        "import tensorflow as tf \n",
        "from tensorflow import keras \n",
        "from tensorflow.keras import Model,Input,layers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import *\n",
        "\n",
        "import tensorflow_hub as hub \n",
        "import tokenizers\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "import tensorflow_text as text \n",
        "\n",
        "# !wget https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\n",
        "# from conlleval import evaluate\n",
        "# import matplotlib.pyplot as plt\n",
        "import string\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from data_loader import load_sentences, list_maker\n",
        "from model import ner_model\n",
        "from helper_functions import tag_encoder, str_maker\n",
        "import platform\n",
        "import sklearn \n",
        "from sklearn.metrics import f1_score,classification_report,confusion_matrix\n",
        "\n",
        "# !pip install --quiet pipreqs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('python version :',platform.python_version())\n",
        "print('regex version :',re.__version__)\n",
        "print('numpy version :', np.__version__)\n",
        "print('pandas version :', pd.__version__)\n",
        "print('tf version :', tf.__version__)\n",
        "print('tf_text version :',text.__version__)\n",
        "print('tf_hub version :',hub.__version__)\n",
        "print('huggingface_tokenizers version :',tokenizers.__version__)\n",
        "print('sklearn version', sklearn.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lE_CEW-Y1dS",
        "outputId": "45f73808-660b-486d-e7da-6ffcb5d35c68"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python version : 3.7.13\n",
            "regex version : 2.2.1\n",
            "numpy version : 1.21.6\n",
            "pandas version : 1.3.5\n",
            "tf version : 2.10.0\n",
            "tf_text version : 2.10.0\n",
            "tf_hub version : 0.12.0\n",
            "huggingface_tokenizers version : 0.12.1\n",
            "sklearn version 1.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_url = 'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3'\n",
        "# bert_url = \"https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/4\"\n",
        "bert_url = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2'\n",
        "# bert_url = \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/2\" empty.ADAM() f1 0.56\n",
        "\n",
        "bert_layer = hub.KerasLayer(bert_url, trainable=True)\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy().decode(\"utf-8\")\n",
        "tokenizer = BertWordPieceTokenizer(vocab=vocab_file, lowercase=False)"
      ],
      "metadata": {
        "id": "e4zoVwU2HdOH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc95cffe-15cf-4d5c-b579-982ceac16dec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set  = load_sentences('drive/MyDrive/Colab Notebooks/datasets/conn2003/train.txt')\n",
        "test_set  = load_sentences('drive/MyDrive/Colab Notebooks/datasets/conn2003/test.txt')\n",
        "validation_set  = load_sentences('drive/MyDrive/Colab Notebooks/datasets/conn2003/valid.txt')\n",
        "\n",
        "print('train_set_length :',len(train_set))\n",
        "print('test_set_length :',len(test_set))\n",
        "print('validation_set_length :',len(validation_set))\n",
        "\n",
        "sentences_train = list_maker(train_set, 0)\n",
        "tags_train = list_maker(train_set, 1)\n",
        "\n",
        "sentences_test = list_maker(test_set, 0)\n",
        "tags_test = list_maker(test_set, 1)\n",
        "\n",
        "sentences_validation = list_maker(validation_set, 0)\n",
        "tags_validation = list_maker(validation_set, 1)\n",
        "\n",
        "\n",
        "j_tags = ' '.join(tags_train)\n",
        "unique_tags = np.unique(j_tags.split())\n",
        "num_tags = len(unique_tags) ### number of unique tags\n",
        "# unique_tags =  '[PAD]' + unique_tags\n",
        "print('number of unique tags :', num_tags)\n",
        "print('unique tags:', unique_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAFj6kyriGkG",
        "outputId": "2f333162-89b5-4441-f9e2-d05bb7b8a228"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_set_length : 14041\n",
            "test_set_length : 3453\n",
            "validation_set_length : 3250\n",
            "number of unique tags : 9\n",
            "unique tags: ['B-LOC' 'B-MISC' 'B-ORG' 'B-PER' 'I-LOC' 'I-MISC' 'I-ORG' 'I-PER' 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_2tags = {i:j for i,j in enumerate(unique_tags)}\n",
        "tags_2enc = {j:i for i,j in enumerate(unique_tags)}"
      ],
      "metadata": {
        "id": "ryjEKvTBiIx3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def token_aligner(sentences_train,tags_train):\n",
        "    var2 = []\n",
        "    for j in range(len(sentences_train)):\n",
        "        encoded_sentence = tokenizer.encode(sentences_train[j])\n",
        "        tags = tags_train[j].split(' ')\n",
        "        counter = 0\n",
        "        var1 = []\n",
        "        for i in range(1, len(encoded_sentence.offsets)- 1):\n",
        "            x = encoded_sentence.offsets[i][0]\n",
        "            y = encoded_sentence.offsets[i-1][1]\n",
        "\n",
        "            if x!=y :\n",
        "                counter +=1\n",
        "                var1.append(tags[counter])\n",
        "            if x == y:\n",
        "                var1.append(tags[counter]) \n",
        "        var2.append(var1)\n",
        "    \n",
        "    return var2"
      ],
      "metadata": {
        "id": "6KnXVBMhiNto"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder(x):\n",
        "    tokenizer = BertWordPieceTokenizer(vocab=vocab_file, lowercase=True)\n",
        "    ids = []\n",
        "    type_ids = []\n",
        "    attention_mask = []\n",
        "    ids_len = []\n",
        "    for i in range(len(x)):\n",
        "        var1 = tokenizer.encode(x[i])\n",
        "        ids.append(var1.ids)\n",
        "        type_ids.append(var1.type_ids)\n",
        "        attention_mask.append(var1.attention_mask)\n",
        "        ids_len.append(len(var1.ids))\n",
        "    return ids, type_ids, attention_mask,ids_len"
      ],
      "metadata": {
        "id": "pt3pERt2iPYX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aligned_tags_train = token_aligner(sentences_train,tags_train)\n",
        "aligned_tags_val = token_aligner(sentences_validation,tags_validation)"
      ],
      "metadata": {
        "id": "f95RQQZniR12"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ids, train_type_ids, train_attention_mask,ids_len = encoder(sentences_train)\n",
        "encoded_tags_train = tag_encoder(tags_train,tags_2enc)"
      ],
      "metadata": {
        "id": "p6BG2I5tiTVP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_ids, val_type_ids, val_attention_mask,ids_len = encoder(sentences_validation)\n",
        "encoded_tags_val = tag_encoder(tags_validation,tags_2enc)"
      ],
      "metadata": {
        "id": "M5ZPfcHuiU_3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_len =128\n",
        "def padder(x,pad_len):\n",
        "    padded_var = pad_sequences(\n",
        "    x,\n",
        "    maxlen=pad_len,\n",
        "    dtype='int32',\n",
        "    padding='post',\n",
        "    truncating='post',\n",
        "    value=0.0\n",
        "    )\n",
        "    return padded_var\n",
        "\n",
        "pad_ids_train = padder(train_ids,input_len)\n",
        "pad_type_ids_train = padder(train_type_ids,input_len)\n",
        "pad_attention_mask_train = padder(train_attention_mask,input_len)\n",
        "pad_tags_train = padder(encoded_tags_train, input_len)\n",
        "\n",
        "\n",
        "pad_ids_val = padder(val_ids,input_len)\n",
        "pad_type_ids_val = padder(val_type_ids,input_len)\n",
        "pad_attention_mask_val = padder(val_attention_mask,input_len)\n",
        "pad_tags_val = padder(encoded_tags_val, input_len)"
      ],
      "metadata": {
        "id": "ByLxBP88jBpz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = hub.KerasLayer(encoder_url)\n",
        "encoder = hub.KerasLayer(bert_url,trainable=True)"
      ],
      "metadata": {
        "id": "Nd6chzLziej4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def ner_model(num_tags):\n",
        "#     encoder_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
        "#     bert_url = \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/2\"\n",
        "# #     bert_url = \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/2\"\n",
        "\n",
        "\n",
        "#     encoder = hub.KerasLayer(bert_url,trainable=True)\n",
        "#     text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n",
        "#     encoder_inputs = preprocessor(text_input)\n",
        "#     outputs = encoder(encoder_inputs)\n",
        "#     sequence_output = outputs[\"sequence_output\"]\n",
        "#     sequence_output = Dropout(0.3)(sequence_output)\n",
        "#     final_layer = Dense(num_tags, activation = 'softmax')(sequence_output)\n",
        "#     return Model(inputs = text_input, outputs = final_layer)\n",
        "bert_ner_model = ner_model(num_tags)"
      ],
      "metadata": {
        "id": "19SDFz9fiWeI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bert_ner_model.compile(\n",
        "#     loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "#     optimizer = tf.keras.optimizers.Adam(),\n",
        "#     metrics = ['accuracy']\n",
        "# )\n",
        "\n",
        "# bert_ner_model.fit(\n",
        "#     np.array(sentences_train),\n",
        "#     pad_tags_train,\n",
        "#     # deneme,\n",
        "#     epochs = 1,\n",
        "#     batch_size = 20\n",
        "# )"
      ],
      "metadata": {
        "id": "CUZu0fp5i6nr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# os.listdir('drive/MyDrive/Colab Notebooks/trained_models/bert_ner_model/model_weights')\n",
        "# model_path = 'drive/MyDrive/Colab Notebooks/trained_models/bert_ner_model/model_weights'\n",
        "# model_name = '/bert_ner_weights'\n",
        "# model_path +model_name\n",
        "# bert_ner_model.save_weights(model_path +model_name)\n",
        "# os.listdir('drive/MyDrive/Colab Notebooks/trained_models/bert_ner_model/model_weights')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcxOmtVQQm-R",
        "outputId": "d3a2a06f-0488-4bc8-eeaa-5caddd3dcd7c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bert_ner_weights.h5',\n",
              " 'bert_ner_weights.data-00000-of-00001',\n",
              " 'bert_ner_weights.index',\n",
              " 'checkpoint']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_ner_model.load_weights('drive/MyDrive/Colab Notebooks/trained_models/bert_ner_model/model_weights/bert_ner_weights.h5')"
      ],
      "metadata": {
        "id": "cB4rveI63g8L"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# new_model = ner_model(num_tags)\n",
        "# new_model.load_weights(model_path +model_name)\n"
      ],
      "metadata": {
        "id": "ba2AKd8HSh64"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = bert_ner_model.predict(np.array(sentences_validation))"
      ],
      "metadata": {
        "id": "oUcOmlB9i6gs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9699bc5a-d32c-4d8c-b2ae-ec3cbf5d169f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "102/102 [==============================] - 48s 466ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top2 = []\n",
        "v2 = []\n",
        "for i in range(len(encoded_tags_val)):\n",
        "    x = encoded_tags_val[i]\n",
        "    v1 = []\n",
        "    top1 = []\n",
        "    for j in range(len(x)):\n",
        "        v1.append(p[i][j])\n",
        "        top1.append(int( tf.math.top_k(p[i][j],k=1)[1]))\n",
        "    v2.append(v1)\n",
        "    top2.append(top1)"
      ],
      "metadata": {
        "id": "gc2ovhX9i6ZF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "f1_score((np.concatenate(encoded_tags_val)), (np.concatenate(top2)), average='micro')"
      ],
      "metadata": {
        "id": "xjLrnlfJoGPM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69bebebf-3a7e-440b-d24f-5c53ba84bb5d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8424126786340095"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(np.concatenate(encoded_tags_val), np.concatenate(top2),\n",
        "                            target_names = ['0','1','2','3','4','5','6','7','8']\n",
        "                            ))\n",
        "\n",
        "conf_mat=confusion_matrix(np.concatenate(encoded_tags_val), np.concatenate(top2))\n",
        "print(conf_mat)"
      ],
      "metadata": {
        "id": "u5KGcgQegDrT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c8d1dbe-86db-4791-f643-82249a263603"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.26      0.72      0.38      1837\n",
            "           1       0.72      0.42      0.53       922\n",
            "           2       0.73      0.54      0.62      1341\n",
            "           3       0.65      0.55      0.60      1842\n",
            "           4       0.50      0.57      0.53       257\n",
            "           5       0.70      0.36      0.48       346\n",
            "           6       0.67      0.45      0.54       751\n",
            "           7       0.71      0.47      0.57      1307\n",
            "           8       0.93      0.90      0.92     42759\n",
            "\n",
            "    accuracy                           0.84     51362\n",
            "   macro avg       0.65      0.55      0.57     51362\n",
            "weighted avg       0.88      0.84      0.85     51362\n",
            "\n",
            "[[ 1325     6    17    16    22     0     6     0   445]\n",
            " [  110   390    22    17     5    10     1     2   365]\n",
            " [   94    15   725   110     2     0    29     4   362]\n",
            " [  190     4     8  1018     1     0     4    71   546]\n",
            " [   39     0     1     0   146     0     3     3    65]\n",
            " [   35    19     3     1    11   125     3     3   146]\n",
            " [   57     3    28     5     4     5   335    22   292]\n",
            " [  144     1     1    57     5     0     1   611   487]\n",
            " [ 3150   100   190   336    97    39   115   139 38593]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z-NAhl043x1y"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}