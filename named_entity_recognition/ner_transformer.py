# -*- coding: utf-8 -*-
"""ner_transformer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BiEeP-8TSsRT2NlAvAwHFnwPWCaPVRWO
"""

# ner_transformer.ipynb

# Code cell <u2HXD-X70QNF>
# #%% [code]
import numpy as np 
import pandas as pd 
from io import StringIO 
from google.colab import drive 
from google.colab import drive
drive.mount('/content/drive')
import os 
import re
import tensorflow as tf 
from tensorflow import keras 
from tensorflow.keras import Model,Input,layers
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import *
!wget https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py
from conlleval import evaluate
import matplotlib.pyplot as plt
import string

EMBED_DIM = 32
num_heads = 2
ff_dim=32
num_tags = 9
max_len = 128
corpus_size = 22000
num_epochs = 1


def load_sentences(filepath):

    final = []
    sentences = []

    with open(filepath, 'r') as f:
        
        for line in f.readlines():
            
            if (line == ('-DOCSTART- -X- -X- O\n') or line == '\n'):
                if len(sentences) > 0:
                    final.append(sentences)
                    sentences = []
            else:
                l = line.split(' ')
                sentences.append((l[0], l[3].strip('\n')))
    
    return final


def list_maker(data_set,col):
    v2 = []

    for i in data_set:
        v1 = []

        for j in i:
            v1.append(j[col])

        v2.append(' '.join(v1))

    return v2

train_set  = load_sentences('drive/MyDrive/Colab Notebooks/datasets/conn2003/train.txt')
test_set  = load_sentences('drive/MyDrive/Colab Notebooks/datasets/conn2003/test.txt')
validation_set  = load_sentences('drive/MyDrive/Colab Notebooks/datasets/conn2003/valid.txt')

print('train_set_length :',len(train_set))
print('test_set_length :',len(test_set))
print('validation_set_length :',len(validation_set))

sentences_train = list_maker(train_set, 0)
tags_train = list_maker(train_set, 1)

sentences_test = list_maker(test_set, 0)
tags_test = list_maker(test_set, 1)

sentences_validation = list_maker(validation_set, 0)
tags_validation = list_maker(validation_set, 1)


j_tags = ' '.join(tags_train)
unique_tags = np.unique(j_tags.split())
num_tags = len(unique_tags) ### number of unique tags

print('number of unique tags :', num_tags)
print('unique tags:', unique_tags)

enc_2tags = {i:j for i,j in enumerate(unique_tags)}
tags_2enc = {j:i for i,j in enumerate(unique_tags)}

def tag_encoder(tags):
    '''
    encoding tags using tag corpus(enc_2tags)
    '''

    encoded_tags = []

    for i in tags:
        t1 = []

        for j in i.split():

            t1.append(tags_2enc[str(j)])
        encoded_tags.append(t1)

    return encoded_tags

def pad_punctuation(s): 
    return re.sub(f"([{string.punctuation}])", r' \1 ', s)


sentences_train = [pad_punctuation(s) for s in sentences_train]
sentences_test = [pad_punctuation(s) for s in sentences_test]
sentences_validation = [pad_punctuation(s) for s in sentences_validation]
tf_tokenizer = Tokenizer(num_words=corpus_size,lower=True,oov_token = '<OOV>')

def sentence_prep(sentence_list):
    tf_tokenizer.fit_on_texts(sentence_list)
    word_index = tf_tokenizer.word_index
    sentence_seqs = tf_tokenizer.texts_to_sequences(sentence_list)
    padded_sentence_seqs = pad_sequences(sentence_seqs,padding = 'post',maxlen = max_len)

    return padded_sentence_seqs,word_index


padded_sentence_seqs_train, word_index_train = sentence_prep(sentences_train)
padded_sentence_seqs_test, word_index_test = sentence_prep(sentences_test)
padded_sentence_seqs_valid, word_index_valid = sentence_prep(sentences_validation)

encoded_tags_train = tag_encoder(tags_train)
encoded_tags_test = tag_encoder(tags_test)
encoded_tags_validation = tag_encoder(tags_validation)

padded_tags_train = pad_sequences(encoded_tags_train,padding = 'post',maxlen = max_len)
padded_tags_test = pad_sequences(encoded_tags_test,padding = 'post',maxlen = max_len)
padded_tags_validation = pad_sequences(encoded_tags_validation,padding = 'post',maxlen = max_len)

# Code cell <SD_Z5pLmz2f5>
# #%% [code]
print('train corpus size', len(word_index_train))
print('test corpus size', len(word_index_test))
print('valid corpus size', len(word_index_valid))

# sentences_train = [pad_punctuation(s) for s in sentences_train]
# sentences_train

def get_pos_encoding_matrix(max_len, d_emb):
    pos_enc = np.array(
        [
            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]
            if pos != 0
            else np.zeros(d_emb)
            for pos in range(max_len)
        ]
    )
    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i
    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1
    return pos_enc

inputs = layers.Input((max_len,), 
                    #   dtype=tf.int64
                      )

word_embeddings = layers.Embedding(corpus_size, EMBED_DIM, name="word_embedding")(inputs)
position_embeddings = layers.Embedding(
        input_dim=max_len,
        output_dim=EMBED_DIM,
        weights=[get_pos_encoding_matrix(max_len, EMBED_DIM)],
        name="position_embedding",
    )(tf.range(start=0, limit=max_len, delta=1))
embeddings = word_embeddings + position_embeddings
enc_out = embeddings
for i in range(4):
    # Multi headed self-attention
    attention_output = layers.MultiHeadAttention(   num_heads=num_heads,
                                                    key_dim=EMBED_DIM
                                                    # key_dim=EMBED_DIM //num_heads,
                                                    # name="encoder_{}/multiheadattention".format(i),
                                                    )(enc_out, enc_out, enc_out)
    attention_output = layers.Dropout(0.1)(attention_output)
    attention_output = layers.LayerNormalization(epsilon=1e-6)(enc_out + attention_output)

    # Feed-forward layer
    ffn = keras.Sequential([
            layers.Dense(ff_dim, activation="relu"),
            layers.Dense(EMBED_DIM),
                            ])
    ffn_output = ffn(attention_output)
    ffn_output = layers.Dropout(0.1)(ffn_output)
    sequence_output = layers.LayerNormalization(epsilon=1e-6)(attention_output + ffn_output)
# enc_out =  tf.keras.layers.Lambda(bert_module)(enc_out)    
# att_layer = MultiHeadAttention(num_heads=num_heads, key_dim=EMBED_DIM)(embeddings, embeddings)
# drop_layer = Dropout(0.1)(att_layer)
# d1_layer = Dense(ff_dim, activation="relu")(drop_layer)
# drop_2_layer = Dropout(0.1)(d1_layer)
# d2_layer = Dense(max_len, activation="softmax")(drop_2_layer)
d2_layer = Dense(num_tags, activation="softmax")(sequence_output)


ner_model = Model(inputs = [inputs], outputs = [d2_layer])

loss_fn = keras.losses.SparseCategoricalCrossentropy(
            # from_logits=True, reduction=keras.losses.Reduction.NONE
        )

ner_model.compile(optimizer="adam",
                  loss=loss_fn,
                  metrics = 'accuracy')


ner_hist = ner_model.fit(padded_sentence_seqs_train,
              padded_tags_train, 
              epochs = num_epochs,
            #   validation_split = 0.2
              )

predictions = ner_model.predict(padded_sentence_seqs_test)

print(len(padded_sentence_seqs_valid))
print(len(encoded_tags_validation))

def non_zero_def(x):
    non_zero_array = []
    for i in range(len(x)):
        non_zero_array.append(int(len(np.where(x[i] != 0)[0])))

    return non_zero_array
non_zero_train = non_zero_def(padded_tags_train)
non_zero_test = non_zero_def(padded_tags_test)
non_zero_valid = non_zero_def(padded_tags_validation)

len(non_zero_valid)

non_zero_pred = []

for i in range(len(predictions)):
    non_zero_pred.append(predictions[i][0:non_zero_test[i]])

# iki = []
# iki_enc = []
# for i in range(len(non_zero_pred)):
#     bir = []
#     bir_enc = []
#     for j in non_zero_pred[i]:
#         x  = np.argmax(j)
#         if x == 0 :
#             x =1
#         bir.append(x)
#         # print(x)
#         bir_enc.append(enc_2tags[x])
#     iki.append(bir)
#     iki_enc.append(bir_enc)


# iki_enc3 = np.concatenate(iki_enc)

# c = []
# for i in tags_test:
#     b = []
#     for k in i.split() :
#         # print(k)
#         b.append(k)

#     c.append(b)   

# evaluate(np.concatenate(c), iki_enc3,verbose = True)

# print(len(np.concatenate(c)))
# print(len(iki_enc3))

for i in range(1,num_tags+1):
    print(i)

# print(len(np.concatenate(c)))
# print(len(iki_enc3))

# val_dataset = (
#     # val_data.map(map_record_to_training_data)
#     .map(lambda x, y: (np.concatenate(c), iki_enc3))
#     # .padded_batch(batch_size)
# )
val_dataset = tf.data.Dataset.from_tensor_slices(
    (padded_sentence_seqs_valid, padded_tags_validation)
)

test_dataset = tf.data.Dataset.from_tensor_slices(
    (padded_sentence_seqs_test, padded_tags_test)
)

def calculate_metrics(dataset):
    all_true_tag_ids, all_predicted_tag_ids = [], []

    for x, y in dataset:
        # x = pad_sequences(np.array(x),padding = 'post', maxlen = 128) 
        # print(x.shape)
        # print(y.shape)
        x = np.array(x).reshape(1,max_len)
        output = ner_model.predict(x)
        predictions = np.argmax(output, axis=-1)
        predictions = np.reshape(predictions, [-1])
        # y = pad_sequences(np.array(y),padding = 'post', maxlen = 128) 
        true_tag_ids = np.reshape(y, [-1])

        mask = (true_tag_ids > 0) & (predictions > 0)
        true_tag_ids = true_tag_ids[mask]
        predicted_tag_ids = predictions[mask]

        all_true_tag_ids.append(true_tag_ids)
        all_predicted_tag_ids.append(predicted_tag_ids)

    all_true_tag_ids = np.concatenate(all_true_tag_ids)
    all_predicted_tag_ids = np.concatenate(all_predicted_tag_ids)

    predicted_tags = [enc_2tags[tag] for tag in all_predicted_tag_ids]
    real_tags = [enc_2tags[tag] for tag in all_true_tag_ids]

    evaluate(real_tags, predicted_tags)
    # evaluate(all_true_tag_ids, all_predicted_tag_ids)


calculate_metrics(test_dataset)

padded_sentence_seqs_valid.shape

padded_tags_validation.shape

padded_sentence_seqs_valid.shape

padded_tags_validation.shape

